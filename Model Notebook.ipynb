{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\master\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\master\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\master\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\master\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\master\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\master\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D , MaxPooling2D , Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 779 images belonging to 5 classes.\n",
      "Found 191 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        'dataset/train',  # this is the target directory\n",
    "        target_size=(64, 64),  # all images will be resized to 64 * 64\n",
    "        batch_size=16,\n",
    "        classes = ['beauty','fashion','home','nutrition','stationary'],\n",
    "        class_mode='categorical'\n",
    "        )\n",
    "\n",
    "# this is a similar generator, for testing data\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        'dataset/test',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=16,\n",
    "        classes = ['beauty','fashion','home','nutrition','stationary'],\n",
    "        class_mode='categorical'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\master\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), input_shape=(64, 64, 3..., activation=\"relu\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\master\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
      "  \n",
      "C:\\Users\\master\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\master\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\master\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=128)`\n",
      "C:\\Users\\master\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=5)`\n"
     ]
    }
   ],
   "source": [
    "classifier=Sequential()\n",
    "\n",
    "classifier.add(Convolution2D(16,3,3,input_shape=(64,64,3),activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "classifier.add(Convolution2D(32,3,3,activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "classifier.add(Convolution2D(64,3,3,activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "classifier.add(Convolution2D(64,3,3,activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "classifier.add(Flatten())\n",
    "\n",
    "classifier.add(Dense(output_dim=128,activation='relu'))\n",
    "classifier.add(Dense(output_dim=5,activation='softmax'))\n",
    "\n",
    "classifier.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\master\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \n",
      "C:\\Users\\master\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=<keras_pre..., verbose=1, steps_per_epoch=48, epochs=30, validation_steps=191)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "19/48 [==========>...................] - ETA: 5s - loss: 1.5799 - acc: 0.2632"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\master\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\PIL\\Image.py:931: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 9s 180ms/step - loss: 1.5144 - acc: 0.3344 - val_loss: 1.3467 - val_acc: 0.4398\n",
      "Epoch 2/30\n",
      "48/48 [==============================] - 8s 161ms/step - loss: 1.3865 - acc: 0.4039 - val_loss: 1.2628 - val_acc: 0.4869\n",
      "Epoch 3/30\n",
      "48/48 [==============================] - 9s 184ms/step - loss: 1.3295 - acc: 0.4457 - val_loss: 1.2503 - val_acc: 0.5183\n",
      "Epoch 4/30\n",
      "48/48 [==============================] - 7s 150ms/step - loss: 1.2948 - acc: 0.5077 - val_loss: 1.2040 - val_acc: 0.4921\n",
      "Epoch 5/30\n",
      "48/48 [==============================] - 9s 194ms/step - loss: 1.1431 - acc: 0.5480 - val_loss: 1.0239 - val_acc: 0.5602\n",
      "Epoch 6/30\n",
      "48/48 [==============================] - 9s 178ms/step - loss: 0.9656 - acc: 0.5985 - val_loss: 1.0364 - val_acc: 0.5812\n",
      "Epoch 7/30\n",
      "48/48 [==============================] - 9s 185ms/step - loss: 0.9616 - acc: 0.6351 - val_loss: 1.0232 - val_acc: 0.6178\n",
      "Epoch 8/30\n",
      "48/48 [==============================] - 7s 139ms/step - loss: 0.8284 - acc: 0.6797 - val_loss: 0.9217 - val_acc: 0.6283\n",
      "Epoch 9/30\n",
      "48/48 [==============================] - 9s 185ms/step - loss: 0.8218 - acc: 0.6852 - val_loss: 0.8836 - val_acc: 0.6597\n",
      "Epoch 10/30\n",
      "48/48 [==============================] - 8s 169ms/step - loss: 0.6810 - acc: 0.7393 - val_loss: 0.9312 - val_acc: 0.6597\n",
      "Epoch 11/30\n",
      "48/48 [==============================] - 9s 195ms/step - loss: 0.6992 - acc: 0.7501 - val_loss: 0.8746 - val_acc: 0.6754\n",
      "Epoch 12/30\n",
      "48/48 [==============================] - 7s 147ms/step - loss: 0.6392 - acc: 0.7604 - val_loss: 0.8140 - val_acc: 0.6859\n",
      "Epoch 13/30\n",
      "48/48 [==============================] - 8s 160ms/step - loss: 0.6031 - acc: 0.7713 - val_loss: 0.7740 - val_acc: 0.7068\n",
      "Epoch 14/30\n",
      "48/48 [==============================] - 9s 178ms/step - loss: 0.5474 - acc: 0.8067 - val_loss: 0.7517 - val_acc: 0.7696\n",
      "Epoch 15/30\n",
      "48/48 [==============================] - 8s 165ms/step - loss: 0.5084 - acc: 0.8132 - val_loss: 0.8056 - val_acc: 0.7330\n",
      "Epoch 16/30\n",
      "48/48 [==============================] - 7s 155ms/step - loss: 0.5683 - acc: 0.7930 - val_loss: 0.7288 - val_acc: 0.7749\n",
      "Epoch 17/30\n",
      "48/48 [==============================] - 8s 159ms/step - loss: 0.4929 - acc: 0.8301 - val_loss: 0.9757 - val_acc: 0.7225\n",
      "Epoch 18/30\n",
      "48/48 [==============================] - 8s 167ms/step - loss: 0.4510 - acc: 0.8342 - val_loss: 0.6973 - val_acc: 0.7644\n",
      "Epoch 19/30\n",
      "48/48 [==============================] - 9s 182ms/step - loss: 0.4220 - acc: 0.8504 - val_loss: 0.9015 - val_acc: 0.7173\n",
      "Epoch 20/30\n",
      "48/48 [==============================] - 9s 193ms/step - loss: 0.4127 - acc: 0.8490 - val_loss: 0.7206 - val_acc: 0.7853\n",
      "Epoch 21/30\n",
      "48/48 [==============================] - 9s 181ms/step - loss: 0.3762 - acc: 0.8751 - val_loss: 0.5910 - val_acc: 0.8063\n",
      "Epoch 22/30\n",
      "48/48 [==============================] - 9s 187ms/step - loss: 0.3258 - acc: 0.8763 - val_loss: 0.8303 - val_acc: 0.7539\n",
      "Epoch 23/30\n",
      "48/48 [==============================] - 8s 161ms/step - loss: 0.3932 - acc: 0.8543 - val_loss: 0.6099 - val_acc: 0.8115\n",
      "Epoch 24/30\n",
      "48/48 [==============================] - 9s 192ms/step - loss: 0.3271 - acc: 0.8815 - val_loss: 0.7699 - val_acc: 0.7749\n",
      "Epoch 25/30\n",
      "48/48 [==============================] - 8s 173ms/step - loss: 0.2625 - acc: 0.9036 - val_loss: 0.7974 - val_acc: 0.7958\n",
      "Epoch 26/30\n",
      "48/48 [==============================] - 8s 167ms/step - loss: 0.3188 - acc: 0.8796 - val_loss: 0.6206 - val_acc: 0.8063\n",
      "Epoch 27/30\n",
      "48/48 [==============================] - 8s 160ms/step - loss: 0.2341 - acc: 0.9155 - val_loss: 0.6263 - val_acc: 0.8272\n",
      "Epoch 28/30\n",
      "48/48 [==============================] - 8s 157ms/step - loss: 0.2397 - acc: 0.9175 - val_loss: 0.7844 - val_acc: 0.7749\n",
      "Epoch 29/30\n",
      "48/48 [==============================] - 9s 187ms/step - loss: 0.2274 - acc: 0.9115 - val_loss: 0.7835 - val_acc: 0.8115\n",
      "Epoch 30/30\n",
      "48/48 [==============================] - 7s 154ms/step - loss: 0.3351 - acc: 0.8699 - val_loss: 0.5987 - val_acc: 0.8272\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b3cfec1b70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit_generator(training_set,\n",
    "                         samples_per_epoch = 779,\n",
    "                         nb_epoch= 30,\n",
    "                         validation_data = test_set,\n",
    "                         nb_val_samples = 191,\n",
    "                         verbose = 1\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('et.jpg')\n",
    "resize = cv2.resize(img,(64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = classifier.predict(np.expand_dims(resize/255, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.0340291e-05, 6.4426118e-01, 2.4390032e-03, 8.0354316e-03,\n",
       "        3.4519404e-01]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this image is a fashion product\n"
     ]
    }
   ],
   "source": [
    "if yhat.argmax() == 0:\n",
    "    print('this image is a beauty product')\n",
    "elif yhat.argmax() == 1:\n",
    "    print('this image is a fashion product')\n",
    "elif yhat.argmax() == 2:\n",
    "    print('this image is a home product')\n",
    "elif yhat.argmax() == 3:\n",
    "    print('this image is a nutrition product')\n",
    "elif yhat.argmax() == 4:\n",
    "    print('this is Stationary product')\n",
    "else:\n",
    "    print('error undefined image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0131353 , 0.00375347, 0.00776086, 0.00196051, 0.9733899 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img2 = cv2.imread('testing2.jpg')\n",
    "resize2 = cv2.resize(img2,(64,64))\n",
    "yhat2 = classifier.predict(np.expand_dims(resize2/255, 0))\n",
    "yhat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is Stationary product\n"
     ]
    }
   ],
   "source": [
    "if yhat2.argmax() == 0:\n",
    "    print('this image is a beauty product')\n",
    "elif yhat2.argmax() == 1:\n",
    "    print('this image is a fashion product')\n",
    "elif yhat2.argmax() == 2:\n",
    "    print('this image is a home product')\n",
    "elif yhat2.argmax() == 3:\n",
    "    print('this image is a nutrition product')\n",
    "elif yhat2.argmax() == 4:\n",
    "    print('this is Stationary product')\n",
    "else:\n",
    "    print('error undefined image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.5623473e-01, 1.2843159e-02, 3.1688850e-02, 2.6350326e-04,\n",
       "        9.8969720e-02]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img3 = cv2.imread('testing3.jpg')\n",
    "resize3 = cv2.resize(img3,(64,64))\n",
    "yhat3 = classifier.predict(np.expand_dims(resize3/255, 0))\n",
    "yhat3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this image is a beauty product\n"
     ]
    }
   ],
   "source": [
    "if yhat3.argmax() == 0:\n",
    "    print('this image is a beauty product')\n",
    "elif yhat3.argmax() == 1:\n",
    "    print('this image is a fashion product')\n",
    "elif yhat3.argmax() == 2:\n",
    "    print('this image is a home product')\n",
    "elif yhat3.argmax() == 3:\n",
    "    print('this image is a nutrition product')\n",
    "elif yhat3.argmax() == 4:\n",
    "    print('this is Stationary product')\n",
    "else:\n",
    "    print('error undefined image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.0517686e-05, 4.3372076e-02, 3.4209323e-04, 9.3056637e-01,\n",
       "        2.5649007e-02]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img4 = cv2.imread('neu.jpg')\n",
    "resize4 = cv2.resize(img4,(64,64))\n",
    "yhat4 = classifier.predict(np.expand_dims(resize4/255, 0))\n",
    "yhat4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this image is a nutrition product\n"
     ]
    }
   ],
   "source": [
    "if yhat4.argmax() == 0:\n",
    "    print('this image is a beauty product')\n",
    "elif yhat4.argmax() == 1:\n",
    "    print('this image is a fashion product')\n",
    "elif yhat4.argmax() == 2:\n",
    "    print('this image is a home product')\n",
    "elif yhat4.argmax() == 3:\n",
    "    print('this image is a nutrition product')\n",
    "elif yhat4.argmax() == 4:\n",
    "    print('this is Stationary product')\n",
    "else:\n",
    "    print('error undefined image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
